{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b10e345-bbad-46cb-a316-4c6a3ea327ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \n",
      "\"Journey to the Stars: 3030\"\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "What's a cool song title for a song about {theme} in the year {year}?\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\", \"year\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Input data for the prompt\n",
    "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
    "\n",
    "# Create LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the LLMChain to get the AI-generated song title\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Theme: interstellar travel\")\n",
    "print(\"Year: 3030\")\n",
    "print(\"AI-generated song title:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a11c0d9-88e6-4668-bca1-bb99da21c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\n",
      "Emotion:  optimism\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
    "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "formatted_prompt = few_shot_prompt.format(input=\"yellow\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
    "response = chain.run({})\n",
    "\n",
    "print(\"Color: yellow\")\n",
    "print(\"Emotion:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09c646-70af-4d81-b030-5c1290b905c3",
   "metadata": {},
   "source": [
    "##### Chain Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5966659d-a9e7-41f4-a56b-c21af2f452c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist: Albert Einstein\n",
      "Fact: \n",
      "Albert Einstein's theory of general relativity is a theory of gravitation that states that the gravitational force between two objects is a result of the curvature of spacetime caused by the presence of mass and energy. It explains the phenomenon of gravity as a result of the warping of space and time by matter and energy.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Prompt 1\n",
    "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
    "Answer: \"\"\"\n",
    "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
    "\n",
    "# Prompt 2\n",
    "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
    "Answer: \"\"\"\n",
    "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
    "\n",
    "# Create the LLMChain for the first prompt\n",
    "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "response_question = chain_question.run({})\n",
    "\n",
    "# Extract the scientist's name from the response\n",
    "scientist = response_question.strip()\n",
    "\n",
    "# Create the LLMChain for the second prompt\n",
    "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
    "\n",
    "# Input data for the second prompt\n",
    "input_data = {\"scientist\": scientist}\n",
    "\n",
    "# Run the LLMChain for the second prompt\n",
    "response_fact = chain_fact.run(input_data)\n",
    "\n",
    "print(\"Scientist:\", scientist)\n",
    "print(\"Fact:\", response_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836cfce-b353-455d-a0d4-7c950d7bc5ff",
   "metadata": {},
   "source": [
    "##### well-structured prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b71b5b-5f50-49fc-81b4-568b2424f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are some tips for improving communication skills?\n",
      "AI Response:  Practice active listening, be mindful of your body language, and be open to constructive feedback.\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the secret to happiness?\",\n",
    "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
    "    }, {\n",
    "        \"query\": \"How can I become more productive?\",\n",
    "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few-shot prompt template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Define the user query\n",
    "user_query = \"What are some tips for improving communication skills?\"\n",
    "\n",
    "# Run the LLMChain for the user query\n",
    "response = chain.run({\"query\": user_query})\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca6913-6410-41bf-8fc0-b0939a307b03",
   "metadata": {},
   "source": [
    "##### FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db816dde-68fe-4663-82b8-cac58a06f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start by studying Schrödinger's cat. That should get you off to a good start.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ae456-c6c2-4ddc-8bf4-1484328d921b",
   "metadata": {},
   "source": [
    "##### Dynamic_prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "626dfe9b-0d16-4a31-8a3b-57c0a36fb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander Graham Bell, the guy who saved us from having to send telegrams all the time.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Existing example and prompt definitions, and dynamic_prompt_template initialization\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  \n",
    ")\n",
    "\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"Who invented the telephone?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c41ca1-1437-4146-819d-e98cdd58671c",
   "metadata": {},
   "source": [
    "##### Human/AI messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd3b4eb-3d00-4d28-b656-9bf72681fdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ahoy matey! Me name be Ethan Grover. I be likin' icehockey, arrr!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"My name is Ethan Grover. I like icehockey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45517deb-4862-41e5-9703-60005227f11f",
   "metadata": {},
   "source": [
    "##### SemanticSimilarityExampleSelector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "220e8619-5354-4f3d-91f0-fdba1e92e374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./deeplake/ already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 5 embeddings in 1 batches of size 5:: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (17, 1536)  float32   None   \n",
      "    id        text      (17, 1)      str     None   \n",
      " metadata     json      (17, 1)      str     None   \n",
      "   text       text      (17, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10°C\n",
      "Output: 50°F\n",
      "\n",
      "Input: 10°C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30°C\n",
      "Output: 86°F\n",
      "\n",
      "Input: 30°C\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (18, 1536)  float32   None   \n",
      "    id        text      (18, 1)      str     None   \n",
      " metadata     json      (18, 1)      str     None   \n",
      "   text       text      (18, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40°C\n",
      "Output: 104°F\n",
      "\n",
      "Input: 40°C\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here.  (by default, org id is your username)\n",
    "my_activeloop_org_id = \"vikasgrover\"\n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector2\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Embedding function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, embeddings, db, k=1\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\", \n",
    "    input_variables=[\"temperature\"],\n",
    ")\n",
    "\n",
    "# Test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
    "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input\n",
    "\n",
    "# Add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362a1d5-d874-406c-bb7d-356fac39bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
